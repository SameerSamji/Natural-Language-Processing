{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1c5cv2w2ImO"
   },
   "source": [
    "# Practical exercise 4 - experiments with Wordnet\n",
    "\n",
    "WordNet is a large digital lexicon made by hand. The kernel of WordNet are the so called synsets that can be understood as meanings. Each word belongs to one or more synsets and each synset is made up of one or more words. Semantic relations like hypernymy and hyponymy exist between synsets, not between words! Consequently, there is no such thing like synonymy in Wordnet. If two words are synonymous the will share one or several synsets.<br>\n",
    "It is possible to access Wordnet is via the web interface: http://wordnetweb.princeton.edu/perl/webwn. There we can see e.g. the synsets\n",
    "of a word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQOh0mujYeT"
   },
   "source": [
    "# 1. WordNet in Python\n",
    "\n",
    "The NLTK package offers some easy methods to access WordNet. Before you use WordNet you have to run once the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHfttgQnjpUi",
    "outputId": "02950680-5e18-4af0-e751-1dbbe71e99ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVsKddF9kVRJ"
   },
   "source": [
    "How to access synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luBhaPNJj9NE",
    "outputId": "93b980a3-749c-4f11-97a5-0fd492b1a03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rock.n.01')\n",
      "Synset('rock.n.02')\n",
      "Synset('rock.n.03')\n",
      "Synset('rock.n.04')\n",
      "Synset('rock_candy.n.01')\n",
      "Synset('rock_'n'_roll.n.01')\n",
      "Synset('rock.n.07')\n",
      "Synset('rock.v.01')\n",
      "Synset('rock.v.02')\n",
      "\n",
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# get synsets of a word\n",
    "synsets = wn.synsets(\"rock\")\n",
    "for s in synsets:\n",
    "    print(s)\n",
    "print()\n",
    "\n",
    "# use synset identifier directly\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "print(dog.hypernyms())\n",
    "print(dog.hyponyms())\n",
    "print(dog.lemmas())  # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZPvJKVys2UW"
   },
   "source": [
    "An easy way to compute the similarity between two synsets is to measure the length of the path between the synsets in the WordNet hierarchy made up by the hypernym relations. The method path_simiarity returns 1/p where p is the length of the path between two synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-SrM7VLrXnp",
    "outputId": "078aa484-d188-4efb-a799-12a232ca7749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between ape and monkey:  0.3333333333333333\n",
      "Similarity between ape and zoo:  0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "ape = wn.synset(\"ape.n.01\")\n",
    "monkey = wn.synset(\"monkey.n.01\")\n",
    "zoo = wn.synset(\"zoo.n.01\")\n",
    "\n",
    "print( \"Similarity between ape and monkey: \", ape.path_similarity(monkey))\n",
    "print( \"Similarity between ape and zoo: \", ape.path_similarity(zoo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6PrfM-RtC8i"
   },
   "source": [
    "Wordnet is not completely connected. The path similarity method therefore assumes a fake root node that connect all parts. The path similarity has the problem that words are less similar if they are part of the hierarchy that is worked out in more detail. In general we would assume that the first divisions at the top of the hierarchy imply large semantic differences, while a division at a very deep position in the hierarchy makes only small semantic distinctions. Therefore some alternative measures have been defined, e.g. the Wu-Palmer similarity and the Leacock-Chodorow similarity (feel free to read up on those measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iIAT5CasVTg",
    "outputId": "0b820964-3883-4eb5-ef8c-dea400e5c369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu−Palmer similarity between ape and monkey:  0.9230769230769231\n",
      "Wu−Palmer similarity between ape and zoo:  0.4\n",
      "Leacock Chodorow similarity between ape and monkey:  2.538973871058276\n",
      "Leacock Chodorow similarity between ape and zoo:  1.072636802264849\n"
     ]
    }
   ],
   "source": [
    "print(\"Wu−Palmer similarity between ape and monkey: \", ape.wup_similarity(monkey))\n",
    "print(\"Wu−Palmer similarity between ape and zoo: \", ape.wup_similarity(zoo))\n",
    "\n",
    "print(\"Leacock Chodorow similarity between ape and monkey: \", ape.lch_similarity(monkey))\n",
    "print(\"Leacock Chodorow similarity between ape and zoo: \", ape.lch_similarity(zoo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz6udeQWtsXT"
   },
   "source": [
    "Both measures give higher weight to distances between nodes that are closer to the root. However, the distance to the root is also a design decision and a number of measures try to include other information sources as well. E.g. the similarity measures of Resnik and Lin include the frequency of words in a corpus as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tTTvM1w48FN"
   },
   "source": [
    "# 2. Exercise:\n",
    "\n",
    "0. Read in the email dataset (see exercise 3). You may copy some of the code from that notebook.\n",
    "\n",
    "1. Let us investigate the coverage of this data in Wordnet:\n",
    "    - Count the unique words (types) in the data and store them in a list.\n",
    "    - How many of those items have synsets in Wordnet? (calculate a percentage value)\n",
    "    - What is the average number of synsets per type?\n",
    "\n",
    "2. Not all words have lexical meaning. We can filter certain word classes. Apply POS-tagging (for example https://www.nltk.org/book/ch05.html) to extract only nouns (just NN - not proper nouns NNP). Check the coverage in Wordnet for these nouns. How many have synsets in Wordnet? (calculate a percentage value)\n",
    "\n",
    "3. Experiments with the similarity of words:\n",
    "    - Choose 10 out of the 50 most frequent nouns from the data set (they all should have at least one synset in Wordnet).\n",
    "    - Now compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words (you may use the first synset for each word for this calculation when words have multiple synsets). You might want to display the resulting numbers in a table. Which words are most similar to each other?\n",
    "    - Check for all sentences which contain the word 'Obama': How often does each of the 10 words you selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQIZYzIy7AHW"
   },
   "source": [
    "#### 0. Reading the Email Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"emails-body.txt.zip\", 'r') as zip_f:\n",
    "    zip_f.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = open('emails-body.txt', encoding='utf8').read().split('<cmail>\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Investigating the coverage of this data in Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count the unique words (types) in the data and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from somajo import SoMaJo\n",
    "\n",
    "somajo_tokenizer = SoMaJo(language=\"en_PTB\",\n",
    "                          split_camel_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for sentence in somajo_tokenizer.tokenize_text(texts):\n",
    "    data_tok.extend([token.text for token in sentence])\n",
    "unique_data_tok = list(set(data_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique words (types) in the Data: 37340\n"
     ]
    }
   ],
   "source": [
    "print('Total Unique words (types) in the Data: {}'.format(len(unique_data_tok)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many of those items have synsets in Wordnet? (calculate a percentage value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_synsets_percentage(word_tokens):\n",
    "    count = 0\n",
    "    for word in word_tokens:\n",
    "        if wn.synsets(word):\n",
    "            count += 1\n",
    "    return (count/len(word_tokens))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Percentage of items having Synsets is: 66.25%\n"
     ]
    }
   ],
   "source": [
    "synsets_per = calculate_synsets_percentage(unique_data_tok)\n",
    "print('Total Percentage of items having Synsets is: {:.2f}%'.format(synsets_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the average number of synsets per type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Break           75\n",
       "break           75\n",
       "breaks          75\n",
       "broken          72\n",
       "cut             70\n",
       "                ..\n",
       "conveners        1\n",
       "HIKERS           1\n",
       "bankrolling      1\n",
       "incoherently     1\n",
       "reignite         1\n",
       "Name: Average Number of Synsets per Type, Length: 24737, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "synsets_per_type = {word: len(wn.synsets(word)) for word in unique_data_tok if len(wn.synsets(word)) > 0}\n",
    "synsets_per_type = dict(sorted(synsets_per_type.items(), key=lambda item: item[1], reverse=True))\n",
    "pd.Series(synsets_per_type, name='Average Number of Synsets per Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Synsets Per Type: 3.10\n"
     ]
    }
   ],
   "source": [
    "average_synsets_type = sum(synsets_per_type.values())/len(unique_data_tok)\n",
    "print('Average Number of Synsets Per Type: {:.2f}'.format(average_synsets_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Not all words have lexical meaning. We can filter certain word classes. Apply POS-tagging to extract only nouns (just NN - not proper nouns NNP). Check the coverage in Wordnet for these nouns. How many have synsets in Wordnet? (calculate a percentage value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_tokens = [word for (word, tag) in nltk.pos_tag(unique_data_tok) if tag == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Percentage of Nouns (NN) having Synsets is: 78.35%\n"
     ]
    }
   ],
   "source": [
    "NN_synsets_per = calculate_synsets_percentage(NN_tokens)\n",
    "print('Total Percentage of Nouns (NN) having Synsets is: {:.2f}%'.format(NN_synsets_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Experiments with the similarity of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose 10 out of the 50 most frequent nouns from the data set (they all should have at least one synset in Wordnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 out of the 50 most frequent nouns are: \n",
      "['call', 'time', 'w', 'get', 'work', 'government', 'today', 'see', 'support', 'right']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data_counter = Counter(data_tok)\n",
    "NN_token_counts = {noun: data_counter[noun] for noun in NN_tokens if len(wn.synsets(noun)) > 0}\n",
    "NN_token_counts = dict(sorted(NN_token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "top_10_nouns = list(NN_token_counts.keys())[:10]\n",
    "print('Top 10 out of the 50 most frequent nouns are: \\n{}'.format(top_10_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words (you may use the first synset for each word for this calculation when words have multiple synsets). You might want to display the resulting numbers in a table. Which words are most similar to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "WP_distances = np.zeros(shape=(10,10))\n",
    "LC_distances = np.zeros(shape=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index1, noun1 in enumerate(top_10_nouns):\n",
    "    n1 = wn.synsets(noun1)[0]\n",
    "    for index2, noun2 in enumerate(top_10_nouns):\n",
    "        WP_distances[index1, index2] = n1.wup_similarity(wn.synsets(noun2)[0])\n",
    "        LC_distances[index1, index2] = n1.lch_similarity(wn.synsets(noun2)[0])\n",
    "WP_distances = pd.DataFrame(WP_distances, columns=top_10_nouns, index=top_10_nouns)\n",
    "LC_distances = pd.DataFrame(LC_distances, columns=top_10_nouns, index=top_10_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(df, similarity_metric):\n",
    "    print('Printing Words Similar to each other using metric: {}\\n'.format(similarity_metric))\n",
    "    for i in range(df.shape[0]):\n",
    "        similar_word = df.iloc[i].nlargest(2).index[-1]\n",
    "        print('{} is Similar to {}'.format(df.index[i],similar_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Wu-Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>time</th>\n",
       "      <th>w</th>\n",
       "      <th>get</th>\n",
       "      <th>work</th>\n",
       "      <th>government</th>\n",
       "      <th>today</th>\n",
       "      <th>see</th>\n",
       "      <th>support</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                call      time         w       get      work  government  \\\n",
       "call        1.000000  0.117647  0.235294  0.086957  0.117647    0.117647   \n",
       "time        0.117647  1.000000  0.266667  0.400000  0.571429    0.285714   \n",
       "w           0.235294  0.266667  1.000000  0.190476  0.266667    0.266667   \n",
       "get         0.086957  0.400000  0.190476  1.000000  0.500000    0.200000   \n",
       "work        0.117647  0.571429  0.266667  0.500000  1.000000    0.285714   \n",
       "government  0.117647  0.285714  0.266667  0.200000  0.285714    1.000000   \n",
       "today       0.125000  0.307692  0.285714  0.210526  0.307692    0.307692   \n",
       "see         0.315789  0.125000  0.250000  0.090909  0.125000    0.125000   \n",
       "support     0.117647  0.571429  0.266667  0.500000  0.857143    0.285714   \n",
       "right       0.105263  0.375000  0.235294  0.272727  0.375000    0.250000   \n",
       "\n",
       "               today       see   support     right  \n",
       "call        0.125000  0.315789  0.117647  0.105263  \n",
       "time        0.307692  0.125000  0.571429  0.375000  \n",
       "w           0.285714  0.250000  0.266667  0.235294  \n",
       "get         0.210526  0.090909  0.500000  0.272727  \n",
       "work        0.307692  0.125000  0.857143  0.375000  \n",
       "government  0.307692  0.125000  0.285714  0.250000  \n",
       "today       1.000000  0.133333  0.307692  0.266667  \n",
       "see         0.133333  1.000000  0.125000  0.111111  \n",
       "support     0.307692  0.125000  1.000000  0.375000  \n",
       "right       0.266667  0.111111  0.375000  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Words Similar to each other using metric: Wu-Palmer\n",
      "\n",
      "call is Similar to see\n",
      "time is Similar to work\n",
      "w is Similar to today\n",
      "get is Similar to work\n",
      "work is Similar to support\n",
      "government is Similar to today\n",
      "today is Similar to time\n",
      "see is Similar to call\n",
      "support is Similar to work\n",
      "right is Similar to time\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(WP_distances, 'Wu-Palmer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Leacock-Chodorow Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>time</th>\n",
       "      <th>w</th>\n",
       "      <th>get</th>\n",
       "      <th>work</th>\n",
       "      <th>government</th>\n",
       "      <th>today</th>\n",
       "      <th>see</th>\n",
       "      <th>support</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>0.546544</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.747214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.864997</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>1.691676</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>1.691676</td>\n",
       "      <td>1.239691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.998529</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.747214</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>0.998529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.546544</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>0.747214</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.593064</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.804373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.864997</td>\n",
       "      <td>1.691676</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>2.538974</td>\n",
       "      <td>1.239691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>0.864997</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.072637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.929536</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>1.152680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.998529</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>0.593064</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.804373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.864997</td>\n",
       "      <td>1.691676</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>2.538974</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>1.239691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.747214</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>3.637586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                call      time         w       get      work  government  \\\n",
       "call        3.637586  0.864997  0.998529  0.546544  0.864997    0.864997   \n",
       "time        0.864997  3.637586  1.152680  1.072637  1.691676    1.239691   \n",
       "w           0.998529  1.152680  3.637586  0.747214  1.152680    1.152680   \n",
       "get         0.546544  1.072637  0.747214  3.637586  1.239691    0.804373   \n",
       "work        0.864997  1.691676  1.152680  1.239691  3.637586    1.239691   \n",
       "government  0.864997  1.239691  1.152680  0.804373  1.239691    3.637586   \n",
       "today       0.929536  1.335001  1.239691  0.864997  1.335001    1.335001   \n",
       "see         0.998529  0.929536  1.072637  0.593064  0.929536    0.929536   \n",
       "support     0.864997  1.691676  1.152680  1.239691  2.538974    1.239691   \n",
       "right       0.747214  1.239691  0.998529  0.804373  1.239691    1.072637   \n",
       "\n",
       "               today       see   support     right  \n",
       "call        0.929536  0.998529  0.864997  0.747214  \n",
       "time        1.335001  0.929536  1.691676  1.239691  \n",
       "w           1.239691  1.072637  1.152680  0.998529  \n",
       "get         0.864997  0.593064  1.239691  0.804373  \n",
       "work        1.335001  0.929536  2.538974  1.239691  \n",
       "government  1.335001  0.929536  1.239691  1.072637  \n",
       "today       3.637586  0.998529  1.335001  1.152680  \n",
       "see         0.998529  3.637586  0.929536  0.804373  \n",
       "support     1.335001  0.929536  3.637586  1.239691  \n",
       "right       1.152680  0.804373  1.239691  3.637586  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LC_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Words Similar to each other using metric: Leacock-Chodorow\n",
      "\n",
      "call is Similar to w\n",
      "time is Similar to work\n",
      "w is Similar to today\n",
      "get is Similar to work\n",
      "work is Similar to support\n",
      "government is Similar to today\n",
      "today is Similar to time\n",
      "see is Similar to w\n",
      "support is Similar to work\n",
      "right is Similar to time\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(LC_distances, 'Leacock-Chodorow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check for all sentences which contain the word 'Obama': How often does each of the 10 words you selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_sentences = []\n",
    "for sentence in texts:\n",
    "    if 'obama' in sentence.lower():\n",
    "        obama_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_obama_count = np.zeros(shape=(len(obama_sentences), 10))\n",
    "for index1, word in enumerate(top_10_nouns):\n",
    "    for index2, ob_sent in enumerate(obama_sentences):\n",
    "        if word in ob_sent.lower():\n",
    "            word_obama_count[index2, index1] += 1\n",
    "word_obama_count = pd.DataFrame(word_obama_count, columns=top_10_nouns, index=['Sentence ' + str(i) for i in range(1, len(obama_sentences) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>time</th>\n",
       "      <th>w</th>\n",
       "      <th>get</th>\n",
       "      <th>work</th>\n",
       "      <th>government</th>\n",
       "      <th>today</th>\n",
       "      <th>see</th>\n",
       "      <th>support</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 206</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 207</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 208</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 209</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 210</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              call  time    w  get  work  government  today  see  support  \\\n",
       "Sentence 1     1.0   1.0  1.0  1.0   1.0         1.0    0.0  0.0      1.0   \n",
       "Sentence 2     1.0   1.0  1.0  1.0   0.0         0.0    1.0  1.0      0.0   \n",
       "Sentence 3     0.0   0.0  1.0  0.0   0.0         0.0    1.0  1.0      0.0   \n",
       "Sentence 4     0.0   0.0  1.0  0.0   0.0         0.0    1.0  1.0      0.0   \n",
       "Sentence 5     0.0   1.0  1.0  1.0   1.0         1.0    0.0  1.0      1.0   \n",
       "...            ...   ...  ...  ...   ...         ...    ...  ...      ...   \n",
       "Sentence 206   0.0   1.0  1.0  1.0   1.0         0.0    1.0  1.0      1.0   \n",
       "Sentence 207   1.0   1.0  1.0  1.0   1.0         0.0    0.0  0.0      0.0   \n",
       "Sentence 208   0.0   1.0  1.0  1.0   1.0         1.0    1.0  1.0      1.0   \n",
       "Sentence 209   0.0   0.0  1.0  1.0   0.0         1.0    0.0  0.0      1.0   \n",
       "Sentence 210   0.0   1.0  1.0  1.0   1.0         0.0    0.0  1.0      1.0   \n",
       "\n",
       "              right  \n",
       "Sentence 1      1.0  \n",
       "Sentence 2      0.0  \n",
       "Sentence 3      0.0  \n",
       "Sentence 4      0.0  \n",
       "Sentence 5      1.0  \n",
       "...             ...  \n",
       "Sentence 206    0.0  \n",
       "Sentence 207    1.0  \n",
       "Sentence 208    1.0  \n",
       "Sentence 209    1.0  \n",
       "Sentence 210    1.0  \n",
       "\n",
       "[210 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_obama_count"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-2022_exercise4_Wordnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
