{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85fad3d-2908-49d2-8403-c487851cc9b8",
   "metadata": {},
   "source": [
    "### Defining two different models for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f415e-d465-42ee-87d9-ebea6f298e40",
   "metadata": {},
   "source": [
    "##### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c002c6-369c-4692-89bb-18ebf60ac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b3d84-f5a8-4f93-b292-ac34156bcf34",
   "metadata": {},
   "source": [
    "##### Model 1 (Glove Twitter):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc067b7-27b6-4fe4-b8c5-e320873b5fe2",
   "metadata": {},
   "source": [
    "This model contains pre-trained glove vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased.\n",
    "\n",
    "**Model Reference:**\n",
    "https://huggingface.co/Gensim/glove-twitter-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1e5159-83f6-4e92-b519-a5011feff25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8bcb8-a56a-4890-9604-2a9c21cd8c03",
   "metadata": {},
   "source": [
    "##### Model 2 (Word2Vec):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33803ec3-ad79-4799-84f1-4c554f67befd",
   "metadata": {},
   "source": [
    "This model contains pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "**Model Reference:**\n",
    "https://huggingface.co/fse/word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe24a3d-f165-428a-834f-0eca9457c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129c11a-19b1-4200-8289-122a44ee13dc",
   "metadata": {},
   "source": [
    "### Constructing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24bc78-bd34-4be7-892d-0a28bb3d19c0",
   "metadata": {},
   "source": [
    "##### Dataset Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d2352-ce98-486f-a3b9-dd00ea4e2c4d",
   "metadata": {},
   "source": [
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger.\n",
    "\n",
    "**Dataset Reference:**\n",
    "https://huggingface.co/datasets/wikitext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd006e2-e39b-4b4a-a6e2-16fddbf389ab",
   "metadata": {},
   "source": [
    "##### Loading the Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0131bd-fd88-413a-a542-1b5aa82901f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the train split of the dataset\n",
    "from datasets import load_dataset\n",
    "dataset_train = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "\n",
    "# Converting the Dataset into a Pandas Dataframe\n",
    "dataset_train = dataset_train.data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae44eb6-0294-4e74-8759-fcce7dcab432",
   "metadata": {},
   "source": [
    "##### Function to Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49d67c9-fa73-4654-922e-4be3f0e60a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    #Converting the text into lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Removing all the punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #Removing all the special characters\n",
    "    text = text.replace(r'[^a-z0-9]', '')\n",
    "    \n",
    "    #Removing all the stop words\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    text = [word for word in nltk.word_tokenize(text) if word not in english_stopwords]\n",
    "    \n",
    "    #Removing words with less than 3 characters\n",
    "    text = [word for word in text if len(word) > 3]\n",
    "    \n",
    "    #Returning only words that are taged as Nouns\n",
    "    return [word for (word, tag) in nltk.pos_tag(text) if tag == 'NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a40df-e5b7-4cf2-b898-38d22fef07d7",
   "metadata": {},
   "source": [
    "##### Preprocessing the Dataset to extract some words for Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cde4813-9c86-43ae-87e8-6efab4bffc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61568"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for index, row in dataset_train.iterrows():\n",
    "    #Checking if the current row contains any data\n",
    "    if len(row.text) > 0:\n",
    "        words.extend(preprocess_data(row.text))\n",
    "    \n",
    "    # Breaking it after extracting sufficient words\n",
    "    if len(words) > 1000000:\n",
    "        break\n",
    "\n",
    "#Creating Counter object to count total occurence of each unique word\n",
    "words_counter = Counter(c for c in words)\n",
    "len(words_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c35ff9-7dac-4f59-819e-29e1591774f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the top 200 words from the Counter object\n",
    "top_200_words = words_counter.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1313da-0c50-4d8c-802f-f31654e85375",
   "metadata": {},
   "source": [
    "##### Appending the synonynms for some words into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf51c28b-b851-4798-b7d1-971a28a3c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2f5175-94a6-4286-9180-2247bb062f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_200_words_list = [word for (word, count) in top_200_words]\n",
    "similar_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9afc52-41ac-4221-9c4a-ffa0c4f4b42a",
   "metadata": {},
   "source": [
    "###### Finding the Synonynms for the first 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "492a8f0d-b13b-47b5-a2d2-b9f45095d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating over the first 50 words\n",
    "for w in top_200_words_list[:50]:\n",
    "    sw = [w]\n",
    "    \n",
    "    #Finding all the synonynms of the current word\n",
    "    synsets = wn.synsets(w)\n",
    "    \n",
    "    for s in synsets:\n",
    "        \n",
    "        #Splitting the string to get only the synonynm word\n",
    "        syn = s.name().split('.')[0]\n",
    "        \n",
    "        #If the synonynm word is not equal to the current word, then only appending it to the list\n",
    "        if syn != w:\n",
    "            sw.append(syn)\n",
    "    \n",
    "    #Checking if we have found any synonynms for the current word\n",
    "    if len(sw) > 1:\n",
    "        similar_words.append(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fa69e-43dc-4591-8cae-b292dec164be",
   "metadata": {},
   "source": [
    "##### Saving the Dataset in the TSV file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b41947a0-94fe-4b75-9dca-6ab5a8a5e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.tsv', 'w', encoding='utf8') as tsvfile:\n",
    "    #Writing into a TCV file\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    writer.writerow(['Words', 'Model1 Word Embedding', 'Model2 Word Embedding'])\n",
    "    \n",
    "    #Writing the word and its embedding with different models\n",
    "    for word in top_200_words_list:\n",
    "        try:\n",
    "            writer.writerow([word, model1.get_vector(word), model2.get_vector(word)])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    #Writing the synonynms word and its embedding with different models\n",
    "    for word_list in similar_words:\n",
    "        for word in word_list:\n",
    "            if word not in top_200_words_list:\n",
    "                try:\n",
    "                    writer.writerow([word, model1.get_vector(word), model2.get_vector(word)])\n",
    "                except Exception as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954fb23-54e2-4c03-8440-af0f048f178e",
   "metadata": {},
   "source": [
    "### Evaluating the Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ada63-fc2f-4364-8a18-83b5671f703d",
   "metadata": {},
   "source": [
    "##### Evaluating Model on Average Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9c241f-b9a1-4fb9-8da0-d2014ae28fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, similar_words):\n",
    "    #Saving the similarity score for each instance\n",
    "    total_similarity_scores = []\n",
    "    \n",
    "    for sw in similar_words:\n",
    "        words_vector = []\n",
    "        \n",
    "        #For each word calculating its word embedding\n",
    "        for word in sw:\n",
    "            try:\n",
    "                words_vector.append(model1.get_vector(word))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        if len(words_vector) <= 1:\n",
    "            continue\n",
    "        \n",
    "        #Calculating the similarity score of the first word with the rest of words\n",
    "        words_vector = np.array(words_vector)\n",
    "        similarity_score = model1.cosine_similarities(words_vector[0], words_vector[1:,:])\n",
    "        total_similarity_scores.append(np.mean(similarity_score))\n",
    "\n",
    "    #Returning the mean of similarity scores\n",
    "    return np.mean(total_similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a4bcfb-7d13-44d9-8cb6-8ef4ed20a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity score on Model 1: 0.5843071341514587\n",
      "Average Similarity score on Model 2: 0.5843071341514587\n"
     ]
    }
   ],
   "source": [
    "print('Average Similarity score on Model 1: {}'.format(evaluate_model(model1, similar_words)))\n",
    "print('Average Similarity score on Model 2: {}'.format(evaluate_model(model2, similar_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
