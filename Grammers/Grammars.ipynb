{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax and Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Classical CFG\n",
    "\n",
    "You might know context free grammars (CFG) from theoretical comouter sciebce, formal languages or compiler construction. Origiannly CFGs were developed to describe natural languages.\n",
    "\n",
    "On the one hand side CFG is a generative model. Starting with a start symbol we can meake context free derivations and thus count all grammatical words (or sentences) of a language (a possible infinite set of words/strings). On the other hand side, for CFGs efficient ($O(n^{2.7})$) parsing algorithms (like Earley, Cock-Kasamy-Younger) exist that can produce parse trees, representing an analysis of a given input.\n",
    "\n",
    "Stanfords NLTK provides a CFG parser taht easily can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK offers a number of different parser generators. See http://www.nltk.org/book/ch08.html for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sent = nltk.word_tokenize('I shot an elephant in my pajamas')\n",
    "\n",
    "parse_trees = list(parser.parse(sent))\n",
    "for tree in parse_trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Ghostscript is installed, you can draw beautifull trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_trees[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_trees[1].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example with recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP\n",
    "  NP -> Det Nom | PropN\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V S | V NP PP\n",
    "  PP -> P NP\n",
    "  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n",
    "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
    "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n",
    "  P -> 'on'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (Nom (Adj angry) (Nom (N bear))))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (Nom (Adj frightened) (Nom (Adj little) (Nom (N squirrel)))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar2)\n",
    "\n",
    "sent1 = nltk.word_tokenize('the angry bear chased the frightened little squirrel')\n",
    "sent2 = nltk.word_tokenize('Chatterer said Buster thought the tree was tall')\n",
    "\n",
    "parse_trees = list(parser.parse(sent1))\n",
    "for tree in parse_trees:\n",
    "    print(tree)\n",
    "    tree.draw() # if you have ghostscript installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can generate sentences, of course. This is not intended to be a usefull mechanism for language generation, but it helps to check to see whether you grammar is complete and not too much overgenerating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buster chased angry\n",
      "Buster chased frightened\n",
      "Buster chased little\n",
      "Buster chased tall\n",
      "Buster saw angry\n",
      "Buster saw frightened\n",
      "Buster saw little\n",
      "Buster saw tall\n",
      "Buster said angry\n",
      "Buster said frightened\n",
      "Buster said little\n",
      "Buster said tall\n",
      "Buster thought angry\n",
      "Buster thought frightened\n",
      "Buster thought little\n",
      "Buster thought tall\n",
      "Buster was angry\n",
      "Buster was frightened\n",
      "Buster was little\n",
      "Buster was tall\n",
      "Buster put angry\n",
      "Buster put frightened\n",
      "Buster put little\n",
      "Buster put tall\n",
      "Chatterer chased angry\n",
      "Chatterer chased frightened\n",
      "Chatterer chased little\n",
      "Chatterer chased tall\n",
      "Chatterer saw angry\n",
      "Chatterer saw frightened\n",
      "Chatterer saw little\n",
      "Chatterer saw tall\n",
      "Chatterer said angry\n",
      "Chatterer said frightened\n",
      "Chatterer said little\n",
      "Chatterer said tall\n",
      "Chatterer thought angry\n",
      "Chatterer thought frightened\n",
      "Chatterer thought little\n",
      "Chatterer thought tall\n",
      "Chatterer was angry\n",
      "Chatterer was frightened\n",
      "Chatterer was little\n",
      "Chatterer was tall\n",
      "Chatterer put angry\n",
      "Chatterer put frightened\n",
      "Chatterer put little\n",
      "Chatterer put tall\n",
      "Joe chased angry\n",
      "Joe chased frightened\n",
      "Joe chased little\n",
      "Joe chased tall\n",
      "Joe saw angry\n",
      "Joe saw frightened\n",
      "Joe saw little\n",
      "Joe saw tall\n",
      "Joe said angry\n",
      "Joe said frightened\n",
      "Joe said little\n",
      "Joe said tall\n",
      "Joe thought angry\n",
      "Joe thought frightened\n",
      "Joe thought little\n",
      "Joe thought tall\n",
      "Joe was angry\n",
      "Joe was frightened\n",
      "Joe was little\n",
      "Joe was tall\n",
      "Joe put angry\n",
      "Joe put frightened\n",
      "Joe put little\n",
      "Joe put tall\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "\n",
    "for sentence in generate(grammar2, depth=4):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically it can be discussed whether a CFG for natural languages can be written or not. Using the simple classical notation that we also used above this is however practicaly unfeasible. E.g. yo need to distinguis between singular and plural nouns, because we can combine singular nouns with *this* and plural nouns with *these*. Thsu we would have to duplicate all rules in which a Noun occurs. \n",
    "\n",
    "A more practical way to handle this is represented by feature grammars. Feature grammars (as presenetd below) are still (weakly) equivalent to CFGs. That means that we could automatically expand all the rules of the grmmar to obtain a CFG generating exactly the same sentences.\n",
    "\n",
    "In a feature grammar the symbols are extended wit a list of features and values. Values iof features also can be feature lists (we should not allow however recursive structures here as that would mean that we would leave the context free world very soon!). I a rule, we cal also use variables for the values. Using the variables we can e.g. enforce that the value of a feature on two symbols has to be the same, e.g. singular in both cases or plural in both cases. Filling in the values for the variables now becomes a central issue in parsing. This process of filling in the variables is called *unification*.\n",
    "\n",
    "Let us consider some simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DP[AGR=[GND='f', NUM='pl', PERS=3]]\n",
      "  (D[AGR=[NUM='pl', PERS=3]] these)\n",
      "  (N[AGR=[GND='f', NUM='pl']] girls))\n"
     ]
    }
   ],
   "source": [
    "from nltk import grammar, parse\n",
    "\n",
    "g = \"\"\"\n",
    "% start DP\n",
    "DP[AGR=?a] -> D[AGR=?a] N[AGR=?a]\n",
    "D[AGR=[NUM=sg, PERS=3]] -> 'this' | 'that'\n",
    "D[AGR=[NUM=pl, PERS=3]] -> 'these' | 'those'\n",
    "D[AGR=[NUM='pl', PERS=1]] -> 'we'\n",
    "D[AGR=[PERS=2]] -> 'you'\n",
    "N[AGR=[NUM=sg, GND='m']] -> 'boy'\n",
    "N[AGR=[NUM='pl', GND='m']] -> 'boys'\n",
    "N[AGR=[NUM='sg', GND='f']] -> 'girl'\n",
    "N[AGR=[NUM='pl', GND='f']] -> 'girls'\n",
    "N[AGR=[NUM='sg']] -> 'student'\n",
    "N[AGR=[NUM='pl']] -> 'students'\n",
    "\"\"\"\n",
    "\n",
    "grammar = grammar.FeatureGrammar.fromstring(g)\n",
    "tokens = 'these girls'.split()\n",
    "parser = parse.FeatureEarleyChartParser(grammar)\n",
    "trees = parser.parse(tokens)\n",
    "for tree in trees: \n",
    "    tree.draw()\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 'these girl'.split()\n",
    "trees = parser.parse(tokens)\n",
    "for tree in trees: \n",
    "    tree.draw()\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chunking \n",
    "\n",
    "Even a feature grammar is very hard to write. Moreover, parsing with CFGs is very problematic if we have non-ideal input (typos, tokenization errors, ungrammatical sentences) or if the grammar does not cover everything. For many purposes we do not need the complete parse trees. Here chunking grammars can provide an alternative. Like regular expressions, a cuncking grammar describes patterns that are searched in the string. NLTH offers a ckunking grammar with the following properties:\n",
    "\n",
    "\n",
    "* Sentence is not requires to be covered by one tree. The algorithm will find partial structures ('chunks') in the sentence.\n",
    "* No recursion possible\n",
    "* Depth of parse trees is delimited\n",
    "* Right hand side of a rule is a regular expression\n",
    "* Also regex can be used for the names of the grammar symbols\n",
    "* The parser requires a list of pairs (word,POS) as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "First we have to find the contextual correct POS for each word. For English we can use the POS tagges from NLTK. Note that there are many POS tagges availabel on the internet. There are considerable differences in the quality of POS Taggers. So inform and test before using a POS Tagger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('angry', 'JJ'), ('bear', 'NN'), ('chased', 'VBD'), ('the', 'DT'), ('frightened', 'JJ'), ('little', 'JJ'), ('squirrel', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#sent1 = nltk.word_tokenize('the angry bear chased the frightened little squirrel')\n",
    "tagged_tokens = nltk.pos_tag(sent1)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "   NP: {<DT>?(<JJ>)*<NN.*>}\n",
    "\"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT angry/JJ bear/NN)\n",
      "  chased/VBD\n",
      "  (NP the/DT frightened/JJ little/JJ squirrel/NN))\n"
     ]
    }
   ],
   "source": [
    "tree = cp.parse(tagged_tokens)\n",
    "print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we extract just the NPs we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('angry', 'JJ'), ('bear', 'NN')]\n",
      "[('the', 'DT'), ('frightened', 'JJ'), ('little', 'JJ'), ('squirrel', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for node in tree:\n",
    "    if isinstance(node, nltk.tree.Tree):               \n",
    "        if node.label() == 'NP':\n",
    "            NP =  node.leaves()\n",
    "            print(NP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension - Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "   NP: {<DT|PRP.*>?(<JJ>)*<NN.*><PP>?}\n",
    "   {<PRP>}\n",
    "   PP: {<IN><NP>}\n",
    "\"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('shot', 'VBP'), ('an', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NN')]\n",
      "(S\n",
      "  (NP I/PRP)\n",
      "  shot/VBP\n",
      "  (NP an/DT elephant/NN)\n",
      "  (PP in/IN (NP my/PRP$ pajamas/NN)))\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(sent)\n",
    "#tagged_tokens = [(w,t.strip('$')) for (w,t) in tagged_tokens]\n",
    "print(tagged_tokens)\n",
    "tree = cp.parse(tagged_tokens)\n",
    "print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learn Chunking\n",
    "\n",
    "writing grammars still is not easy. Chunking also can easily be learned from annotated data. To do so we hav to reformulate the chunkin problem a little bit. Since we do not allow recursion, we could simply annotate each word with the phrase it belongs to. Since e.g. two NPs might be adjacent, we also need to mark the first word of a phrase. Thus for each phrase type we have two tags. Typically B-tag (begin) and I-tag (inside) for the subsequent words of the phrase. So, if we want to find NPs we have B-NP and I-NP. For the remaining words we use O (outside). This tagging scheme is called IOB-tagging.\n",
    "\n",
    "Of course we need annotated data. A well-known corpus of texts with IOB tags is the CONLL corpus of texts from the Wall Street Journal. This corpus can be downloaded here: http://www.cnts.ua.ac.be/conll2000/chunking/\n",
    "\n",
    "In Python we can easily use many different classifiers. We now only take very simple classifiers that use only one feature per word. As input we now have a list of characteristics. The result is a list of IOB tags. As training data, we now need lists of pairs consisting of a feature for a word and the IOB tag that goes with it. We write a function that reads the corpus and returns such lists. As features we take the parts of speech that are also present in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 'B-NP'), ('IN', 'B-PP'), ('DT', 'B-NP'), ('NN', 'I-NP'), ('VBZ', 'B-VP'), ('RB', 'I-VP'), ('VBN', 'I-VP'), ('TO', 'I-VP'), ('VB', 'I-VP'), ('DT', 'B-NP'), ('JJ', 'I-NP'), ('NN', 'I-NP'), ('IN', 'B-SBAR'), ('NN', 'B-NP'), ('NNS', 'I-NP'), ('IN', 'B-PP'), ('NNP', 'B-NP'), (',', 'O'), ('JJ', 'B-ADJP'), ('IN', 'B-PP'), ('NN', 'B-NP'), ('NN', 'B-NP'), (',', 'O'), ('VB', 'B-VP'), ('TO', 'I-VP'), ('VB', 'I-VP'), ('DT', 'B-NP'), ('JJ', 'I-NP'), ('NN', 'I-NP'), ('IN', 'B-PP'), ('NNP', 'B-NP'), ('CC', 'I-NP'), ('NNP', 'I-NP'), ('POS', 'B-NP'), ('JJ', 'I-NP'), ('NNS', 'I-NP'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def read_conll(filepath):\n",
    "    result = []\n",
    "    file = open(filepath)\n",
    "    sentence = []\n",
    "    for line in file:\n",
    "        line = line.strip('\\n')\n",
    "        if not line.strip(' '):\n",
    "            result.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        (word,pos,tag) = line.split(' ')\n",
    "        sentence.append((pos,tag))\n",
    "    return result\n",
    "    \n",
    "\n",
    "conll_train = read_conll('train.txt')\n",
    "conll_test = read_conll('test.txt')\n",
    "\n",
    "print(conll_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest tagger just takes the most likely tag for each POS. We call this tagger a unigram tagger, because it bases its decision always on a sequence of 1 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tag\n",
    "\n",
    "u_chunker = nltk.tag.UnigramTagger(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Less', 'JJR', 'B-NP'),\n",
      " ('than', 'IN', 'B-PP'),\n",
      " ('three', 'CD', 'I-NP'),\n",
      " ('days', 'NNS', 'I-NP'),\n",
      " ('before', 'IN', 'B-PP'),\n",
      " ('President', 'NNP', 'I-NP'),\n",
      " ('Obama', 'NNP', 'I-NP'),\n",
      " ('turns', 'VBZ', 'B-VP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('keys', 'NNS', 'I-NP'),\n",
      " ('to', 'TO', 'B-PP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('White', 'NNP', 'I-NP'),\n",
      " ('House', 'NNP', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('nuclear', 'JJ', 'I-NP'),\n",
      " ('codes', 'NNS', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('over', 'IN', 'B-PP'),\n",
      " ('to', 'TO', 'B-PP'),\n",
      " ('President-elect', 'JJ', 'I-NP'),\n",
      " ('Donald', 'NNP', 'I-NP'),\n",
      " ('J.', 'NNP', 'I-NP'),\n",
      " ('Trump', 'NNP', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('Mr.', 'NNP', 'I-NP'),\n",
      " ('Trump', 'NNP', 'I-NP'),\n",
      " ('’', 'NNP', 'I-NP'),\n",
      " ('s', 'POS', 'B-NP'),\n",
      " ('transition', 'NN', 'I-NP'),\n",
      " ('staff', 'NN', 'I-NP'),\n",
      " ('has', 'VBZ', 'B-VP'),\n",
      " ('barely', 'RB', 'B-ADVP'),\n",
      " ('engaged', 'VBN', 'I-VP'),\n",
      " ('with', 'IN', 'B-PP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('National', 'NNP', 'I-NP'),\n",
      " ('Security', 'NNP', 'I-NP'),\n",
      " ('Council', 'NNP', 'I-NP'),\n",
      " ('below', 'IN', 'B-PP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('most', 'RBS', 'B-NP'),\n",
      " ('senior', 'JJ', 'I-NP'),\n",
      " ('levels', 'NNS', 'I-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sent = u'Less than three days before President Obama turns the keys to the White House, and the nuclear codes, over to President-elect Donald J. Trump, Mr. Trump’s transition staff has barely engaged with the National Security Council below the most senior levels.'\n",
    "\n",
    "tokenized_sent = nltk.tokenize.word_tokenize(sent)\n",
    "pos_tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "#print(pos_tagged_sent)\n",
    "pos_tags = [t for (w,t) in pos_tagged_sent]\n",
    "#print(pos_tags)\n",
    "chunks = u_chunker.tag(pos_tags)\n",
    "chunked_sent = [(w,p,t) for (w,(p,t)) in zip(tokenized_sent,chunks)]\n",
    "pprint(chunked_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is as expected: not particularly useful. Nevertheless, many tags are correct. NLTK provides a function to evaluate the chunker with the sentences from the test corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_chunker.evaluate(conll_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem is that the sequence of I and B tags is hopeless. The BigramTagger also considers the predicted tag of the previous word as a feature. So the probabilities of sequences of two words, so-called bigrams, are optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8869915781919496"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_chunker = nltk.tag.BigramTagger(conll_train)\n",
    "b_chunker.evaluate(conll_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little trick we can improve the results a bit: if the bigram chunker cannot decide between two possibilities, the unigram chunker should decide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8905164953458429"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bu_chunker = nltk.tag.BigramTagger(conll_train, backoff=u_chunker)\n",
    "bu_chunker.evaluate(conll_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram tagger optimizes the probabilities only locally. A Hidden Markov Model optimizes the probability of the complete sequence of tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_trainer = nltk.tag.HiddenMarkovModelTrainer()\n",
    "hmm_model = hmm_trainer.train_supervised(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050383097283492"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_model.evaluate(conll_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can be improved a little bit by using Conditional Random Fields insteatd of a HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nltk.tag.CRFTagger()\n",
    "ct.train(conll_train,'crf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163940308588555"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ct.set_model_file('crf.model')\n",
    "ct.evaluate(conll_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect, what the result for our test sentence now looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Less', 'JJR', 'B-NP'),\n",
      " ('than', 'IN', 'B-PP'),\n",
      " ('three', 'CD', 'B-NP'),\n",
      " ('days', 'NNS', 'I-NP'),\n",
      " ('before', 'IN', 'B-PP'),\n",
      " ('President', 'NNP', 'B-NP'),\n",
      " ('Obama', 'NNP', 'I-NP'),\n",
      " ('turns', 'VBZ', 'B-VP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('keys', 'NNS', 'I-NP'),\n",
      " ('to', 'TO', 'B-VP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('White', 'NNP', 'I-NP'),\n",
      " ('House', 'NNP', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('nuclear', 'JJ', 'I-NP'),\n",
      " ('codes', 'NNS', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('over', 'IN', 'B-PP'),\n",
      " ('to', 'TO', 'B-PP'),\n",
      " ('President-elect', 'JJ', 'B-NP'),\n",
      " ('Donald', 'NNP', 'I-NP'),\n",
      " ('J.', 'NNP', 'I-NP'),\n",
      " ('Trump', 'NNP', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('Mr.', 'NNP', 'B-NP'),\n",
      " ('Trump', 'NNP', 'I-NP'),\n",
      " ('’', 'NNP', 'I-NP'),\n",
      " ('s', 'POS', 'B-NP'),\n",
      " ('transition', 'NN', 'I-NP'),\n",
      " ('staff', 'NN', 'I-NP'),\n",
      " ('has', 'VBZ', 'B-VP'),\n",
      " ('barely', 'RB', 'I-VP'),\n",
      " ('engaged', 'VBN', 'I-VP'),\n",
      " ('with', 'IN', 'B-PP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('National', 'NNP', 'I-NP'),\n",
      " ('Security', 'NNP', 'I-NP'),\n",
      " ('Council', 'NNP', 'I-NP'),\n",
      " ('below', 'IN', 'B-PP'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('most', 'RBS', 'I-NP'),\n",
      " ('senior', 'JJ', 'I-NP'),\n",
      " ('levels', 'NNS', 'I-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "chunks = bu_chunker.tag(pos_tags)\n",
    "chunked_sent = [(w,p,t) for (w,(p,t)) in zip(tokenized_sent,chunks)]\n",
    "pprint(chunked_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that NLTK also provides a function to build the tree structure as used above from the IOB format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Less/JJR)\n",
      "  (PP than/IN)\n",
      "  (NP three/CD days/NNS)\n",
      "  (PP before/IN)\n",
      "  (NP President/NNP Obama/NNP)\n",
      "  (VP turns/VBZ)\n",
      "  (NP the/DT keys/NNS)\n",
      "  (VP to/TO)\n",
      "  (NP the/DT White/NNP House/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (NP the/DT nuclear/JJ codes/NNS)\n",
      "  ,/,\n",
      "  (PP over/IN)\n",
      "  (PP to/TO)\n",
      "  (NP President-elect/JJ Donald/NNP J./NNP Trump/NNP)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Trump/NNP ’/NNP)\n",
      "  (NP s/POS transition/NN staff/NN)\n",
      "  (VP has/VBZ barely/RB engaged/VBN)\n",
      "  (PP with/IN)\n",
      "  (NP the/DT National/NNP Security/NNP Council/NNP)\n",
      "  (PP below/IN)\n",
      "  (NP the/DT most/RBS senior/JJ levels/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = nltk.chunk.conlltags2tree(chunked_sent)\n",
    "#tree.draw()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way around is also possible (and practical for the excercise!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Less', 'JJR', 'B-NP'),\n",
       " ('than', 'IN', 'B-PP'),\n",
       " ('three', 'CD', 'B-NP'),\n",
       " ('days', 'NNS', 'I-NP'),\n",
       " ('before', 'IN', 'B-PP'),\n",
       " ('President', 'NNP', 'B-NP'),\n",
       " ('Obama', 'NNP', 'I-NP'),\n",
       " ('turns', 'VBZ', 'B-VP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('keys', 'NNS', 'I-NP'),\n",
       " ('to', 'TO', 'B-VP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('White', 'NNP', 'I-NP'),\n",
       " ('House', 'NNP', 'I-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('nuclear', 'JJ', 'I-NP'),\n",
       " ('codes', 'NNS', 'I-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('over', 'IN', 'B-PP'),\n",
       " ('to', 'TO', 'B-PP'),\n",
       " ('President-elect', 'JJ', 'B-NP'),\n",
       " ('Donald', 'NNP', 'I-NP'),\n",
       " ('J.', 'NNP', 'I-NP'),\n",
       " ('Trump', 'NNP', 'I-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('Mr.', 'NNP', 'B-NP'),\n",
       " ('Trump', 'NNP', 'I-NP'),\n",
       " ('’', 'NNP', 'I-NP'),\n",
       " ('s', 'POS', 'B-NP'),\n",
       " ('transition', 'NN', 'I-NP'),\n",
       " ('staff', 'NN', 'I-NP'),\n",
       " ('has', 'VBZ', 'B-VP'),\n",
       " ('barely', 'RB', 'I-VP'),\n",
       " ('engaged', 'VBN', 'I-VP'),\n",
       " ('with', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('National', 'NNP', 'I-NP'),\n",
       " ('Security', 'NNP', 'I-NP'),\n",
       " ('Council', 'NNP', 'I-NP'),\n",
       " ('below', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('most', 'RBS', 'I-NP'),\n",
       " ('senior', 'JJ', 'I-NP'),\n",
       " ('levels', 'NNS', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.chunk.tree2conlltags(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take the CONLL-Train data and extract from each sentence all chunks and the sequence of tags that make up that chunk.\n",
    "2. For each type of chunk, count how often each pattern occurs.\n",
    "3. Use these patterns to construct a regex chunking grammar by hand. \n",
    "4. Evaluate your grammar on the CONLL test data.\n",
    "\n",
    "**Tip**: start with just a very small number of simple rules and test your grammar. Then continue adding more rules and making rules more complex. Always check, whether addition improve your grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Take the CONLL-Train data and extract from each sentence all chunks and the sequence of tags that make up that chunk.\n",
    "##### 2. For each type of chunk, count how often each pattern occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dic(dic, lst):\n",
    "    item = str(lst)\n",
    "    if item in dic:\n",
    "        dic[item] += 1\n",
    "    else:\n",
    "        dic[item] = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chunks = {}\n",
    "for sen in conll_train:\n",
    "    current_pos_chunk = []\n",
    "    for pt, ct in sen:\n",
    "        if ct.startswith('B') or ct.startswith('O'):\n",
    "            if len(current_pos_chunk) > 0:\n",
    "                pos_chunks = update_dic(pos_chunks, current_pos_chunk)\n",
    "            current_pos_chunk = []\n",
    "        if not ct.startswith('O'):\n",
    "            current_pos_chunk.append(pt)\n",
    "pos_chunks = dict(sorted(pos_chunks.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total different types of Chunks in training set: 2787\n"
     ]
    }
   ],
   "source": [
    "print('Total different types of Chunks in training set: {}'.format(len(pos_chunks.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most occured Chunk patterns includes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"['IN']\", 21533),\n",
       " (\"['DT', 'NN']\", 7226),\n",
       " (\"['VBD']\", 5290),\n",
       " (\"['PRP']\", 3805),\n",
       " (\"['NN']\", 3551),\n",
       " (\"['NNS']\", 3408),\n",
       " (\"['NNP']\", 3276),\n",
       " (\"['VBZ']\", 3195),\n",
       " (\"['RB']\", 2893),\n",
       " (\"['NNP', 'NNP']\", 2640),\n",
       " (\"['DT', 'JJ', 'NN']\", 2119),\n",
       " (\"['TO']\", 2015),\n",
       " (\"['JJ', 'NNS']\", 1717),\n",
       " (\"['VBG']\", 1711),\n",
       " (\"['TO', 'VB']\", 1675),\n",
       " (\"['VBP']\", 1658),\n",
       " (\"['JJ']\", 1463),\n",
       " (\"['MD', 'VB']\", 1250),\n",
       " (\"['VBN']\", 1221),\n",
       " (\"['DT', 'NNS']\", 1173)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 20 most occured Chunk patterns includes:')\n",
    "list(pos_chunks.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Use these patterns to construct a regex chunking grammar by hand.\n",
    "##### 4. Evaluate your grammar on the CONLL test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating grammar Regex: \n",
      "NP: {<DT>?<JJ>*<NN><PP>?}\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  34.2%%\n",
      "    Precision:     45.3%%\n",
      "    Recall:        13.7%%\n",
      "    F-Measure:     21.1%%\n",
      "Evaluating grammar Regex: \n",
      "VP: {<VB.*><RB.?>?<NP|PP|TO|VB.*>*}\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  28.2%%\n",
      "    Precision:     65.4%%\n",
      "    Recall:        14.8%%\n",
      "    F-Measure:     24.1%%\n",
      "Evaluating grammar Regex: \n",
      "VP:{<NP>?<VB.*><RB.?>?}\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  25.2%%\n",
      "    Precision:     41.6%%\n",
      "    Recall:        11.8%%\n",
      "    F-Measure:     18.4%%\n"
     ]
    }
   ],
   "source": [
    "grammer_regex = ['NP: {<DT>?<JJ>*<NN><PP>?}',\n",
    "                 'VP: {<VB.*><RB.?>?<NP|PP|TO|VB.*>*}',\n",
    "                 'VP:{<NP>?<VB.*><RB.?>?}']\n",
    "\n",
    "s = conll2000.chunked_sents('test.txt')\n",
    "for gr in grammer_regex:\n",
    "    print('Evaluating grammar Regex: \\n{}'.format(gr))\n",
    "    regexp = nltk.RegexpParser(gr)\n",
    "    print(regexp.evaluate(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
